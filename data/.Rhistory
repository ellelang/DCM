# Generate Utilities
utilities <- xmatrix %*% betadis + errors
utilities
# Generate Choice decision
choicefun <- function (u) {
maxindex <- which.max(u)
u[maxindex] <- 1
u[-maxindex] <- 0
return (u)
}
#######################One occasion data generation
datasetgenfun <- function (n, alt, k){ # n is the number of our sample. n = 1000
X <- matrix(NA,alt*n,k)
Y <- vector(length=alt*n)
# E
id <- rep(1:1000, each=alt)
index <- seq(1, alt*n, alt)
for (i in 1 : n) {
e <- matrix (qnorm(runif(k)), nrow = k, ncol = 1)
betadis <- betamean + gam %*% e
error.new <- InverseFun_TYPE1 (alt)
X.new <- Xgenfun (alt)
utilities <- X.new %*% betadis + error.new
Y[seq(index[i],index[i]+(alt-1))] <- choicefun (utilities)
X[seq(index[i],index[i]+(alt-1)),] <- X.new
}
dataset <- data.frame (id,Y,X)
return (dataset)
}
# 4 occasions
dataset1round <- datasetgenfun(n = 1000, alt = 4, k = 3)
dataset2round <- datasetgenfun(n = 1000, alt = 4, k = 3)
dataset3round <- datasetgenfun(n = 1000, alt = 4, k = 3)
dataset4round <- datasetgenfun(n = 1000, alt = 4, k = 3)
#use the rbind() to generate the dataset of 4 occasions
datawhole_temp <- rbind (dataset1round,dataset2round,dataset3round,dataset4round)
#The index of the occasion
occasion <- rep(1:4, each=4000)
datawhole <- cbind (occasion, datawhole_temp)
# The current dataset is ordered by occasion since we use rbind() to stack the 4 occasions' datasets
head(datawhole,20)
# Re-rank the dataset by the id of each person instead of occasions
datawhole <- datawhole[order(datawhole$id),]
head(datawhole,20)
# Use glm() to test if the dataset make sense
fitlogit <- glm(Y~X1+X2+X3,data=datawhole,family=binomial(link = "logit"))
fitlogit
# Save the data
write.csv (x = datawhole, file= "fp4data.csv", row.names = FALSE)
#############################      Part II: Estimation    #############################
#######################################################################################
#######################################################################################
rm(list =ls())
dataset <- read.csv (file = "fp4data.csv")
head(dataset,20)
est_start <- c( 1,1,1,1.5,1.5,1.5,1.5,1.5,1.5)
index <- seq(1, 16000, 16)
length(index)
#################Repeated Random paramters conditional logit model
my.random_loglikelihood.lik <- function (D,B) {
set.seed(9731177)
LogLK <- vector(length = 1000)
X <- D[,c(4,5,6)]
Y <- D[,3]
bmean <- matrix(B[1:3],nrow = 3, ncol=1) # mean of Beta  dim 3x1
gama_matrix <- matrix (0,ncol = 3,nrow = 3) # Variance - Covariance matrix of Beta dim 3x3
gama_matrix[1,1] <- B[4]
gama_matrix[2,1] <- B[5]
gama_matrix[3,1] <- B[6]
gama_matrix[2,2] <- B[7]
gama_matrix[3,2] <- B[8]
gama_matrix[3,3] <- B[9]
for (i in 1 : 1000) {
e <- matrix (qnorm(runif(3)), nrow = 3, ncol = 1) # e follow the normal distribution
B_hat <- bmean + gama_matrix %*% e # B = B_mean + gama%*% e dim 3x1
y <- as.matrix(Y[seq(index[i],index[i]+15)]) # for each i, y is 16X1
xmatrix <- as.matrix(X[seq(index[i],index[i]+15),])  # for each i, X is 16X3
R <- 100 # Replications of the simulator
pvector <- vector(length = R) # vector of pi
for (r in 1:R){
p_occasion <- vector (length = 4)
for (t in 1 : 4){ # Repeated occasions: 4 occasions
occasion_index <- seq (1,16,4)
xmatrix_occasion <- xmatrix[seq(occasion_index[t],occasion_index[t]+3),]
y_occasion <- as.matrix(y[seq(occasion_index[t],occasion_index[t]+3),])
V <- xmatrix_occasion %*% B_hat # 4X1
sumexp <- sum(exp(V))
yselect <- which( y_occasion == 1)
selectedexp <- exp(xmatrix_occasion[yselect,]%*% B_hat)
p_occasion[t] <- selectedexp / sumexp # The probability of choices in kth occasion
}
pvector[r] <- prod(p_occasion) # The probability of repeated choices in jth repication
}
p <- mean(pvector) # The probability of choice for R replications
LogLK[i] <- log(p)
}
sumLogLK <- -sum (LogLK)
#print( pvector )
return (sumLogLK)
}
#my.random_loglikelihood.lik (dataset,est_start)
modelest <- optim(est_start, my.random_loglikelihood.lik, method = "BFGS", control=list(trace=TRUE), hessian = TRUE, D= dataset)
rm(list = ls())
#############################   Part I: Data Generation   #############################
#######################################################################################
#######################################################################################
set.seed(777777)
# Generate e~N(0,1)
e <- matrix (qnorm(runif(3)), nrow = 3, ncol = 1)
e
# Betas hypothesis
## betas vary across the survey sample (different people have different betas)
## for each person, betas are the same across alternatives and occasions
betamean <- c(.5, 1.5, 2.5)
betamean
## Lower triangular Choleski factor matrix
gam <- matrix (c(1.5,0,0,-1,1.5,0,1,1,1.5),nrow = 3,ncol = 3,byrow = TRUE)
gam
## generate the random betas by beta_mean + Gama %*% e
betadis <-betamean + gam %*% e
betadis
#Generate x
Xgenfun<- function (alt) {
xmatrix <- t(rbind(runif(alt),runif(alt)^2,sqrt(runif(alt))))
return(xmatrix)
}
xmatrix <- Xgenfun(4)
xmatrix
#Generate errors
InverseFun_TYPE1 <- function (number) {
u <- runif (number)
x_type <- -log (-log (u))
return (x_type)
}
errors <- InverseFun_TYPE1 (4)
errors
# Generate Utilities
utilities <- xmatrix %*% betadis + errors
utilities
# Generate Choice decision
choicefun <- function (u) {
maxindex <- which.max(u)
u[maxindex] <- 1
u[-maxindex] <- 0
return (u)
}
#######################One occasion data generation
datasetgenfun <- function (n, alt, k){ # n is the number of our sample. n = 1000
X <- matrix(NA,alt*n,k)
Y <- vector(length=alt*n)
# E
id <- rep(1:1000, each=alt)
index <- seq(1, alt*n, alt)
for (i in 1 : n) {
e <- matrix (qnorm(runif(k)), nrow = k, ncol = 1)
betadis <- betamean + gam %*% e
error.new <- InverseFun_TYPE1 (alt)
X.new <- Xgenfun (alt)
utilities <- X.new %*% betadis + error.new
Y[seq(index[i],index[i]+(alt-1))] <- choicefun (utilities)
X[seq(index[i],index[i]+(alt-1)),] <- X.new
}
dataset <- data.frame (id,Y,X)
return (dataset)
}
# 4 occasions
dataset1round <- datasetgenfun(n = 1000, alt = 4, k = 3)
dataset2round <- datasetgenfun(n = 1000, alt = 4, k = 3)
dataset3round <- datasetgenfun(n = 1000, alt = 4, k = 3)
dataset4round <- datasetgenfun(n = 1000, alt = 4, k = 3)
#use the rbind() to generate the dataset of 4 occasions
datawhole_temp <- rbind (dataset1round,dataset2round,dataset3round,dataset4round)
#The index of the occasion
occasion <- rep(1:4, each=4000)
datawhole <- cbind (occasion, datawhole_temp)
# The current dataset is ordered by occasion since we use rbind() to stack the 4 occasions' datasets
head(datawhole,20)
# Re-rank the dataset by the id of each person instead of occasions
datawhole <- datawhole[order(datawhole$id),]
head(datawhole,20)
# Use glm() to test if the dataset make sense
fitlogit <- glm(Y~X1+X2+X3,data=datawhole,family=binomial(link = "logit"))
fitlogit
# Save the data
write.csv (x = datawhole, file= "fp4data.csv", row.names = FALSE)
#############################      Part II: Estimation    #############################
#######################################################################################
#######################################################################################
rm(list =ls())
dataset <- read.csv (file = "fp4data.csv")
head(dataset,20)
est_start <- c( 1,1,1,1.5,1.5,1.5,1.5,1.5,1.5)
index <- seq(1, 16000, 16)
length(index)
#################Repeated Random paramters conditional logit model
my.random_loglikelihood.lik <- function (D,B) {
set.seed(9731177)
LogLK <- vector(length = 1000)
X <- D[,c(4,5,6)]
Y <- D[,3]
bmean <- matrix(B[1:3],nrow = 3, ncol=1) # mean of Beta  dim 3x1
gama_matrix <- matrix (0,ncol = 3,nrow = 3) # Variance - Covariance matrix of Beta dim 3x3
gama_matrix[1,1] <- B[4]
gama_matrix[2,1] <- B[5]
gama_matrix[3,1] <- B[6]
gama_matrix[2,2] <- B[7]
gama_matrix[3,2] <- B[8]
gama_matrix[3,3] <- B[9]
for (i in 1 : 1000) {
e <- matrix (qnorm(runif(3)), nrow = 3, ncol = 1) # e follow the normal distribution
B_hat <- bmean + gama_matrix %*% e # B = B_mean + gama%*% e dim 3x1
y <- as.matrix(Y[seq(index[i],index[i]+15)]) # for each i, y is 16X1
xmatrix <- as.matrix(X[seq(index[i],index[i]+15),])  # for each i, X is 16X3
R <- 100 # Replications of the simulator
pvector <- vector(length = R) # vector of pi
for (r in 1:R){
p_occasion <- vector (length = 4)
for (t in 1 : 4){ # Repeated occasions: 4 occasions
occasion_index <- seq (1,16,4)
xmatrix_occasion <- xmatrix[seq(occasion_index[t],occasion_index[t]+3),]
y_occasion <- as.matrix(y[seq(occasion_index[t],occasion_index[t]+3),])
V <- xmatrix_occasion %*% B_hat # 4X1
sumexp <- sum(exp(V))
yselect <- which( y_occasion == 1)
selectedexp <- exp(xmatrix_occasion[yselect,]%*% B_hat)
p_occasion[t] <- selectedexp / sumexp # The probability of choices in kth occasion
}
pvector[r] <- prod(p_occasion) # The probability of repeated choices in jth repication
}
p <- mean(pvector) # The probability of choice for R replications
LogLK[i] <- log(p)
}
sumLogLK <- -sum (LogLK)
#print( pvector )
return (sumLogLK)
}
#my.random_loglikelihood.lik (dataset,est_start)
modelest <- optim(est_start, my.random_loglikelihood.lik, method = "BFGS", control=list(trace=TRUE), hessian = TRUE, D= dataset)
modelest$par
rm(list =ls())
dataset <- read.csv (file = "fp4data.csv")
head(dataset,20)
est_start <- c( 1,1,1,1.5,1.5,1.5,1.5,1.5,1.5)
index <- seq(1, 16000, 16)
length(index)
my.random_loglikelihood.lik <- function (D,B) {
set.seed(9731177)
LogLK <- vector(length = 1000)
X <- D[,c(4,5,6)]
Y <- D[,3]
bmean <- matrix(B[1:3],nrow = 3, ncol=1) # mean of Beta  dim 3x1
gama_matrix <- matrix (0,ncol = 3,nrow = 3) # Variance - Covariance matrix of Beta dim 3x3
gama_matrix[1,1] <- B[4]
gama_matrix[2,1] <- B[5]
gama_matrix[3,1] <- B[6]
gama_matrix[2,2] <- B[7]
gama_matrix[3,2] <- B[8]
gama_matrix[3,3] <- B[9]
for (i in 1 : 1000) {
y <- as.matrix(Y[seq(index[i],index[i]+15)]) # for each i, y is 16X1
xmatrix <- as.matrix(X[seq(index[i],index[i]+15),])  # for each i, X is 16X3
R <- 100 # Replications of the simulator
pvector <- vector(length = R) # vector of pi
for (r in 1:R){
p_occasion <- vector (length = 4)
for (t in 1 : 4){ # Repeated occasions: 4 occasions
occasion_index <- seq (1,16,4)
e <- matrix (qnorm(runif(3)), nrow = 3, ncol = 1) # e follow the normal distribution
B_hat <- bmean + gama_matrix %*% e # B = B_mean + gama%*% e dim 3x1
xmatrix_occasion <- xmatrix[seq(occasion_index[t],occasion_index[t]+3),]
y_occasion <- as.matrix(y[seq(occasion_index[t],occasion_index[t]+3),])
V <- xmatrix_occasion %*% B_hat # 4X1
sumexp <- sum(exp(V))
yselect <- which( y_occasion == 1)
selectedexp <- exp(xmatrix_occasion[yselect,]%*% B_hat)
p_occasion[t] <- selectedexp / sumexp # The probability of choices in kth occasion
}
pvector[r] <- prod(p_occasion) # The probability of repeated choices in jth repication
}
p <- mean(pvector) # The probability of choice for R replications
LogLK[i] <- log(p)
}
sumLogLK <- -sum (LogLK)
#print( pvector )
return (sumLogLK)
}
modelest <- optim(est_start, my.random_loglikelihood.lik, method = "BFGS", control=list(trace=TRUE), hessian = TRUE, D= dataset)
rm(list =ls())
dataset <- read.csv (file = "fp4data.csv")
head(dataset,20)
est_start <- c( 1,1,1,1.5,1.5,1.5,1.5,1.5,1.5)
index <- seq(1, 16000, 16)
length(index)
#################Repeated Random paramters conditional logit model
my.random_loglikelihood.lik <- function (D,B) {
set.seed(9731177)
LogLK <- vector(length = 1000)
X <- D[,c(4,5,6)]
Y <- D[,3]
bmean <- matrix(B[1:3],nrow = 3, ncol=1) # mean of Beta  dim 3x1
gama_matrix <- matrix (0,ncol = 3,nrow = 3) # Variance - Covariance matrix of Beta dim 3x3
gama_matrix[1,1] <- B[4]
gama_matrix[2,1] <- B[5]
gama_matrix[3,1] <- B[6]
gama_matrix[2,2] <- B[7]
gama_matrix[3,2] <- B[8]
gama_matrix[3,3] <- B[9]
for (i in 1 : 1000) {
y <- as.matrix(Y[seq(index[i],index[i]+15)]) # for each i, y is 16X1
xmatrix <- as.matrix(X[seq(index[i],index[i]+15),])  # for each i, X is 16X3
R <- 20 # Replications of the simulator
pvector <- vector(length = R) # vector of pi
for (r in 1:R){
p_occasion <- vector (length = 4)
for (t in 1 : 4){ # Repeated occasions: 4 occasions
occasion_index <- seq (1,16,4)
e <- matrix (qnorm(runif(3)), nrow = 3, ncol = 1) # e follow the normal distribution
B_hat <- bmean + gama_matrix %*% e # B = B_mean + gama%*% e dim 3x1
xmatrix_occasion <- xmatrix[seq(occasion_index[t],occasion_index[t]+3),]
y_occasion <- as.matrix(y[seq(occasion_index[t],occasion_index[t]+3),])
V <- xmatrix_occasion %*% B_hat # 4X1
sumexp <- sum(exp(V))
yselect <- which( y_occasion == 1)
selectedexp <- exp(xmatrix_occasion[yselect,]%*% B_hat)
p_occasion[t] <- selectedexp / sumexp # The probability of choices in kth occasion
}
pvector[r] <- prod(p_occasion) # The probability of repeated choices in jth repication
}
p <- mean(pvector) # The probability of choice for R replications
LogLK[i] <- log(p)
}
sumLogLK <- -sum (LogLK)
#print( pvector )
return (sumLogLK)
}
#my.random_loglikelihood.lik (dataset,est_start)
modelest <- optim(est_start, my.random_loglikelihood.lik, method = "BFGS", control=list(trace=TRUE), hessian = TRUE, D= dataset)
modelest$par
#############################      Part II: Estimation    #############################
#######################################################################################
#######################################################################################
rm(list =ls())
dataset <- read.csv (file = "fp4data.csv")
head(dataset,20)
est_start <- c( 1,1,1,1.5,1.5,1.5,1.5,1.5,1.5)
index <- seq(1, 16000, 16)
length(index)
#################Repeated Random paramters conditional logit model
my.random_loglikelihood.lik <- function (D,B) {
set.seed(9731177)
LogLK <- vector(length = 1000)
X <- D[,c(4,5,6)]
Y <- D[,3]
bmean <- matrix(B[1:3],nrow = 3, ncol=1) # mean of Beta  dim 3x1
gama_matrix <- matrix (0,ncol = 3,nrow = 3) # Variance - Covariance matrix of Beta dim 3x3
gama_matrix[1,1] <- B[4]
gama_matrix[2,1] <- B[5]
gama_matrix[3,1] <- B[6]
gama_matrix[2,2] <- B[7]
gama_matrix[3,2] <- B[8]
gama_matrix[3,3] <- B[9]
for (i in 1 : 1000) {
y <- as.matrix(Y[seq(index[i],index[i]+15)]) # for each i, y is 16X1
xmatrix <- as.matrix(X[seq(index[i],index[i]+15),])  # for each i, X is 16X3
e <- matrix (qnorm(runif(3)), nrow = 3, ncol = 1) # e follow the normal distribution
B_hat <- bmean + gama_matrix %*% e # B = B_mean + gama%*% e dim 3x1
R <- 20 # Replications of the simulator
pvector <- vector(length = R) # vector of pi
for (r in 1:R){
p_occasion <- vector (length = 4)
for (t in 1 : 4){ # Repeated occasions: 4 occasions
occasion_index <- seq (1,16,4)
xmatrix_occasion <- xmatrix[seq(occasion_index[t],occasion_index[t]+3),]
y_occasion <- as.matrix(y[seq(occasion_index[t],occasion_index[t]+3),])
V <- xmatrix_occasion %*% B_hat # 4X1
sumexp <- sum(exp(V))
yselect <- which( y_occasion == 1)
selectedexp <- exp(xmatrix_occasion[yselect,]%*% B_hat)
p_occasion[t] <- selectedexp / sumexp # The probability of choices in kth occasion
}
pvector[r] <- prod(p_occasion) # The probability of repeated choices in jth repication
}
p <- mean(pvector) # The probability of choice for R replications
LogLK[i] <- log(p)
}
sumLogLK <- -sum (LogLK)
#print( pvector )
return (sumLogLK)
}
#my.random_loglikelihood.lik (dataset,est_start)
modelest <- optim(est_start, my.random_loglikelihood.lik, method = "BFGS", control=list(trace=TRUE), hessian = TRUE, D= dataset)
modelest$par
rm(list = ls())
setwd("C:/Users/langzx/Desktop/github/DCM/data")
library(mlogit)
library(gmnl)
library(tidyverse)
library(fastDummies)
dataset <- read.csv (file = "wholeCEscio_dataset_envinfo_1119.csv",header = TRUE)
dim(dataset)
dataset$alti <- ifelse(dataset$Alternatives == "V",1,2)
dataset$task <- dataset$ChoiceSet
dataset$ChoiceSet <- 2
names(dataset)
dataset$num_rotation <- as.numeric(dataset$primaryrotation)
#dataset$num_rotation <- replace_na(dataset$num_rotation, -999) not necessary
dataset$convCS <- ifelse(dataset$num_rotation == 1 | dataset$num_rotation == 2, 1, 0)
dataset$corn <- ifelse(dataset$num_rotation == 1 | dataset$num_rotation == 2
| dataset$num_rotation == 4 | dataset$num_rotation == 5
| dataset$num_rotation == 9, 1, 0)
dataset <- plyr::rename(dataset, c("cashrental_peracre" = "cashrent" ,"crp2018" = "crp2018", "dem_president_2016" = "demPrez16",
"incomefromfarming" = "incfar" ,"unemploymentrate" = "unemploy",
"Yearly.Cost" = "costlive", "Hourly.Wage" = "hrwage", "av_monthly_Child.Care" = "childcar",
"av_monthly_Food" = "foodcost", "av_montly_Health.Care" = "healthc",
"av_monthly_Housing" = "housecos", "av_monthly_Transport" = "transpor",
"av_monthly_Other" = "othercos", "av_monthly_Taxes" = "taxcost",
"landvalue_peracre" = "landvalu", "Stream_miles" = "impstrea",
"Lake_acres" = "implakes", "Wetland_acres" = "impwetl", "total_impaired" = "imptotal",
"landarea" = "areaf"))
# note: should make dummies for farm size categories, farm income and others
# to be incorporated and imported into Nlogit
export_test_nlogit <- dplyr::select(dataset, c("id","Y","ChoiceSet","alti", "task",
"Wetland","Payment","Covercrop","NuMgt",
"income", "areaf", "demPrez16", "dem_2018",
"unemploy", "costlive", "hrwage", "taxcost",
"cashrent","crp2018", "landvalu",
"impstrea", "implakes", "impwetl",
"imptotal", "childcar", "foodcost", "healthc",  "housecos",
"othercos", "convCS", "corn"))
export_test_nlogit <- fastDummies::dummy_cols(export_test_nlogit, select_columns = c("income","areaf"))
write.csv(export_test_nlogit,file = "wta_observables11192018.csv", row.names = FALSE)
rm(list = ls())
setwd("C:/Users/langzx/Desktop/github/DCM/data")
library(mlogit)
library(gmnl)
library(tidyverse)
library(fastDummies)
dataset <- read.csv (file = "wholeCEscio_dataset_envinfo_1119.csv",header = TRUE)
dim(dataset)
dataset$alti <- ifelse(dataset$Alternatives == "V",1,2)
dataset$task <- dataset$ChoiceSet
dataset$ChoiceSet <- 2
names(dataset)
dataset$num_rotation <- as.numeric(dataset$primaryrotation)
#dataset$num_rotation <- replace_na(dataset$num_rotation, -999) not necessary
dataset$convCS <- ifelse(dataset$num_rotation == 1 | dataset$num_rotation == 2, 1, 0)
dataset$corn <- ifelse(dataset$num_rotation == 1 | dataset$num_rotation == 2
| dataset$num_rotation == 4 | dataset$num_rotation == 5
| dataset$num_rotation == 9, 1, 0)
dataset <- plyr::rename(dataset, c("cashrental_peracre" = "cashrent" ,"crp2018" = "crp2018", "dem_president_2016" = "demPrez16",
"incomefromfarming" = "incfar" ,"unemploymentrate" = "unemploy",
"Yearly.Cost" = "costlive", "Hourly.Wage" = "hrwage", "av_monthly_Child.Care" = "childcar",
"av_monthly_Food" = "foodcost", "av_montly_Health.Care" = "healthc",
"av_monthly_Housing" = "housecos", "av_monthly_Transport" = "transpor",
"av_monthly_Other" = "othercos", "av_monthly_Taxes" = "taxcost",
"landvalue_peracre" = "landvalu", "Stream_miles" = "impstrea",
"Lake_acres" = "implakes", "Wetland_acres" = "impwetl", "total_impaired" = "imptotal",
"landarea" = "areaf"))
# note: should make dummies for farm size categories, farm income and others
# to be incorporated and imported into Nlogit
export_test_nlogit <- dplyr::select(dataset, c("id","Y","ChoiceSet","alti", "task",
"Wetland","Payment","Covercrop","NuMgt",
"income", "areaf", "demPrez16", "dem_2018",
"unemploy", "costlive", "hrwage", "taxcost",
"cashrent","crp2018", "landvalu",
"impstrea", "implakes", "impwetl",
"imptotal", "childcar", "foodcost", "healthc",  "housecos",
"othercos", "convCS", "corn"))
export_test_nlogit <- fastDummies::dummy_cols(export_test_nlogit, select_columns = c("income","areaf"))
write.csv(export_test_nlogit,file = "wta_observables11192018.csv", row.names = FALSE)
install.packages("fastDummies")
rm(list = ls())
setwd("C:/Users/langzx/Desktop/github/DCM/data")
library(mlogit)
library(gmnl)
library(tidyverse)
library(fastDummies)
dataset <- read.csv (file = "wholeCEscio_dataset_envinfo_1119.csv",header = TRUE)
dim(dataset)
dataset$alti <- ifelse(dataset$Alternatives == "V",1,2)
dataset$task <- dataset$ChoiceSet
dataset$ChoiceSet <- 2
names(dataset)
dataset$num_rotation <- as.numeric(dataset$primaryrotation)
#dataset$num_rotation <- replace_na(dataset$num_rotation, -999) not necessary
dataset$convCS <- ifelse(dataset$num_rotation == 1 | dataset$num_rotation == 2, 1, 0)
dataset$corn <- ifelse(dataset$num_rotation == 1 | dataset$num_rotation == 2
| dataset$num_rotation == 4 | dataset$num_rotation == 5
| dataset$num_rotation == 9, 1, 0)
dataset <- plyr::rename(dataset, c("cashrental_peracre" = "cashrent" ,"crp2018" = "crp2018", "dem_president_2016" = "demPrez16",
"incomefromfarming" = "incfar" ,"unemploymentrate" = "unemploy",
"Yearly.Cost" = "costlive", "Hourly.Wage" = "hrwage", "av_monthly_Child.Care" = "childcar",
"av_monthly_Food" = "foodcost", "av_montly_Health.Care" = "healthc",
"av_monthly_Housing" = "housecos", "av_monthly_Transport" = "transpor",
"av_monthly_Other" = "othercos", "av_monthly_Taxes" = "taxcost",
"landvalue_peracre" = "landvalu", "Stream_miles" = "impstrea",
"Lake_acres" = "implakes", "Wetland_acres" = "impwetl", "total_impaired" = "imptotal",
"landarea" = "areaf"))
# note: should make dummies for farm size categories, farm income and others
# to be incorporated and imported into Nlogit
export_test_nlogit <- dplyr::select(dataset, c("id","Y","ChoiceSet","alti", "task",
"Wetland","Payment","Covercrop","NuMgt",
"income", "areaf", "demPrez16", "dem_2018",
"unemploy", "costlive", "hrwage", "taxcost",
"cashrent","crp2018", "landvalu",
"impstrea", "implakes", "impwetl",
"imptotal", "childcar", "foodcost", "healthc",  "housecos",
"othercos", "convCS", "corn"))
export_test_nlogit <- fastDummies::dummy_cols(export_test_nlogit, select_columns = c("income","areaf"))
write.csv(export_test_nlogit,file = "wta_observables11192018.csv", row.names = FALSE)
dim(export_test_nlogit)
colnames(export_test_nlogit)
